\chapter{Parallel processing in Madagascar}

Modern computers provide parallel computing facilities in the form of
either multiple processing units on one node or multiple distributed
nodes in a cluster. By taking advantage of parallel architectures,
many computational tasks can be significantly accelerated.

A particularly simple yet practically important case is \emph{data
  parallel} algorithms, where the input data can be split into
parallel chunks, processed in parallel by serial algorithms and then
accumulated back into one data stream. Madagascar provides a number of
convenient tools for running serial code into a data parallel fashion
without the need to resolve to low-level programming.

\section{Posix threads}

\section{OpenMP programming and sfomp}

OpenMP (from \emph{Open Multi-Processing}) is an application
programming interface (API) that simplifies parallel tasks on
shared-memory architectures \cite[]{chandra,chapman}. OpenMP appeared
in 1997 (with specifications for Fortran) and is currently supported
by most modern C/C++ and Fortran compilers. The GCC compiler has been
providing support to OpenMP since version 4.2, which appeared in
2007. Madagascar detects the existence of OpenMP at the installation
configuration stage and uses sets the compiler flags appropriately. [FLAGS]

To use OpenMP in your C or Fortran program, you can use special
compiler directives. For example, the following piece of code from
\texttt{\$RSFSRC/user/psava/fdutil.c} uses the \texttt{pragma omp}
directive to instruct the OpenMP-enabled compiler to execute the loop
in parallel.

\lstset{language=c,numbers=left,numberstyle=\tiny,showstringspaces=false}
\lstinputlisting[firstline=592,lastline=601,frame=single]{\RSF/user/psava/fdutil.c}

To simplify OpenMP-based processing for data-parallel tasks,
Madagascar provides \texttt{sfomp} utility. Suppose, for example, that
the input file \texttt{input.rsf} contains a number of traces, which
we want to process in parallel with a bandpass filter. The serial execution
\begin{verbatim}
< input.rsf sfbandpass fhi=50 > output.rsf
\end{verbatim}
can be made parallel by inserting \texttt{sfomp} before the command, as follows:
\begin{verbatim}
< input.rsf sfomp sfbandpass fhi=50 > output.rsf
\end{verbatim}

By default, \texttt{sfomp} uses the number of threads equivalent to
the number of available cores. To use a different number, you can set
\texttt{OMP\_NUM\_THREADS} environmental variable.

By default, \texttt{sfomp} splits the input over the last dimension,
processes different parts of the data, and concatenates the outputs
along the same dimension. To change this behavior, you can use
\texttt{split=} and \texttt{join=} parameters. For example, to
transpose a 2-D dataset in parallel, we can split it over one
dimension and join over the other dimension:
\begin{verbatim}
< original.rsf sfomp sftransp split=1 join=2 > transp.rsf
\end{verbatim}
The result should be equivalent to that of running
\begin{verbatim}
< original.rsf sfomp sftransp split=2 join=1 > transp.rsf
\end{verbatim}
If, instead of concatenating, the results need to be added together,
use \texttt{join=0}.

Certain programs have multiple input or output files. If some of the
input files need to be split similarly to the file coming from the
standard input, use an underscore in front of the parameter name. For
example, the \texttt{sfnmo} program takes the NMO velocity as an extra
input. If the serial execution is
\begin{verbatim}
< cmp.rsf sfnmo velocity=vel.rsf > nmo.rsf
\end{verbatim}
the corresponding parallel run will be
\begin{verbatim}
< cmp.rsf sfomp sfnmo _velocity=vel.rsf > nmo.rsf
\end{verbatim}
provided that \texttt{vel.rsf} can be split along the last axis
similarly to \texttt{cmp.rsf}.

SCons processing makes it easier to handle different cases by
modifying the \texttt{Flow} command. The three examples above may
appear as follows in an \texttt{SConstruct} file:

\lstset{language=python,showstringspaces=false,frame=single}
\begin{lstlisting}
Flow('output','input','bandpass fhi=50',
     split=[2,'omp'])
Flow('transp','original','transp',
     split=[2,'omp'],join='cat axis=1')
Flow('nmo','cmp velocity','nmo velocity=${SOURCES[1]}',
     split=[3,'omp',[0,1]])
\end{lstlisting}

The first parameter in the \texttt{split=} list indicates the axis to
split, the second parameter tells SCons to user \texttt{sfomp} for the
splitting, and the third (optional) parameter lists the numbers of the
source files that need splitting (starting from zero). Splitting all
of the source files is the default behavior, therefore the
\texttt{[0,1]} list in the last example is actually not needed.

\section{MPI programming and sfmpi}

MPI (from \emph{Message Passing Interface}) is a popular
system/protocol for parallel application on both shared-memory and
distributed-memory architectures \cite[]{pacheco,gropp}. There are several alternative
implementations of MPI. The most popular open-source implementations
are MPICH\footnote{\url{https://www.mpich.org/}} and Open
MPI\footnote{\url{https://www.open-mpi.org/}}. Madagascar detects the
existence of MPI at the installation configuration stage. 

To simplify MPI-based processing for data-parallel tasks, Madagascar
provides \texttt{sfmpi} utility. The functionality of \texttt{sfmpi}
is largely similar to that of \texttt{sfomp} but the computation can
be distributed across multiple nodes. For example, running a bandpass
filter in parallel by splitting the input data over the slowest
dimension and using 16 nodes can be done with
\begin{verbatim}
mpirun -np 10 sfmpi sfbandpass fhi=50 --input=input.rsf --output=output.rsf
\end{verbatim}
On different systems, \texttt{mpirun} might be replaced with another
command launching an MPI process. In Madagascar's configuration
(\texttt{\$RSFROOT/share/madagascar/etc/config.py}), this command is
specified by \texttt{MPIRUN=} configuration variable.

Similarly to \texttt{sfomp}, \texttt{sfmpi} splits the input over the last dimension,
processes different parts of the data, and concatenates the outputs
along the same dimension. To change this behavior, you can use
\texttt{split=} and \texttt{join=} parameters. For example, to
transpose a 2-D dataset in parallel, we can split it over one
dimension and join over the other dimension:
\begin{verbatim}
mpirun -np 10 sfmpi sftransp --input=orig.rsf split=1 join=2 --output=transp.rsf
\end{verbatim}
If, instead of concatenating, the results should be added together, use \texttt{join=0}.

While \texttt{sfomp} is restricted to shared memory, \texttt{sfmpi}
can be used more flexibly on both shared-memory and distributed-memory
systems. Another difference is that the threads specified by
\texttt{-np} parameters are treated differently: one of the nodes is
reserved as the ``head node'' for handling input and output, and the
computation is performed on the remaining nodes. In the example above
(\texttt{mpirun -np 10}), 9 nodes are used for the computation while
one node is reserved for input/output.

SCons processing looks similar to the OMP case but replacing \texttt{omp} with \texttt{mpi}:
\lstset{language=python,showstringspaces=false,frame=single}
\begin{lstlisting}
Flow('output','input','bandpass fhi=50',
     split=[2,'mpi'])
Flow('transp','original','transp',
     split=[2,'mpi'],join='cat axis=1')
Flow('nmo','cmp velocity','nmo velocity=${SOURCES[1]}',
     split=[3,'mpi',[0,1]])
\end{lstlisting}

%\section{GPU programming}

\section{Parallel processing in SCons: pscons}

One of the powerful features of SCons is its ability to perform
parallel processing. If different targets can be generated
independently from one another, the build can proceed in
parallel. Suppose, for example, that the \texttt{SConstruct} contains
a loop, where different slices are extracted from a 3-D cube.
\begin{lstlisting}
for slice in range(10):
    Flow('slice'+str(slice),'cube','window n3=1 f3=%d' % slice)
\end{lstlisting}  
Because different slices can be generated independently, SCons can do it in parallel if run with the \texttt{-j} flag:
\begin{verbatim}
scons -j 10
\end{verbatim}
The number after \texttt{-j} corresponds to the number of parallel
threads. Madagascar provides a simple script called \texttt{pscons} (for
``parallel scons''), which makes the task easier: \texttt{pscons}
figures out the number of cores on the current node and runs
\texttt{scons -j} using that number.

In the case of data-parallel processing, independent targets may not
exist initially. However, we can create them by breaking the input
data into different files and processing them separately...

Pros and cons (new targets, data tolerance)...

\section{Parallel processing on shared clusters: sfbatch}

\bibliographystyle{seg}
\bibliography{parallel}

